{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5119ee14",
   "metadata": {},
   "source": [
    "![xkcd](https://imgs.xkcd.com/comics/i_could_care_less.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1af5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recursive Neural Networks\n",
    "\n",
    "- Language models from scratch\n",
    "    - As we saw in the IMDB example, building the language model is the hard part.\n",
    "    - There we used a pre-trained model, let's take a step toward understanding what it does.\n",
    "- Traditional neural networks have a fixed input size and a fixed output size\n",
    "    - e.g. number of pixels in an image is input and number of classes is the output\n",
    "- RNNs not take in all of the input in at once\n",
    "    - Consume a sequence of tokens\n",
    "    - Like having a for loop in your network\n",
    "  \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/defreez/cs356-notebooks/blob/main/notebooks/rnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a243ce39",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "from fastbook import *\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924950e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The human numbers dataset\n",
    "\n",
    "```\n",
    "fifteen \n",
    "sixteen \n",
    "seventeen \n",
    "eighteen \n",
    "nineteen \n",
    "twenty\n",
    "twenty one\n",
    "twenty two\n",
    "twenty three\n",
    "twenty four\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb910a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The human numbers dataset\n",
    "\n",
    "- Nice small and easy dataset\n",
    "- Used in FastAI book\n",
    "- Two text files\n",
    "    - Training data is first 7999 numbers (0 to 7999)\n",
    "    - Validation data is next 1999 numbers (8001 to 9999)\n",
    "    - What happend to eight thousand? Who knows :shrug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "72b94085",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('/root/.fastai/data/human_numbers/train.txt'),Path('/root/.fastai/data/human_numbers/valid.txt')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn_path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "hn_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3403f206",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The FastAI custom List type\n",
    "lines = L()\n",
    "\n",
    "# Open train.txt and append all of the lines in the file to variable lines.\n",
    "with open(hn_path/'train.txt') as f:\n",
    "    lines += L(f.readlines())\n",
    "    \n",
    "# Open valid.txt and append all of the lines in the file to variable lines\n",
    "with open(hn_path/'valid.txt') as f:\n",
    "    lines += L(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1fe36c27",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c8264b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "- In thise case the tokenizer can be very simple\n",
    "- We do want to make sure that we have a marker between tokens\n",
    "- In this case the book uses `.` \n",
    "    - Similar to `xxbos` that is used by the real FastAI language loader\n",
    "    \n",
    "    \n",
    "Example Tokens:\n",
    "```\n",
    "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cc112ed5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use . as separator\n",
    "hn_text = ' . '.join([l.strip() for l in lines])\n",
    "hn_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0a94e5f1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize. For this dataset we don't need anything complicated.\n",
    "tokens = hn_text.split(' ')\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a44ad347",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, ['seventeen', 'hundred', 'ten', 'fifty', 'eighteen'])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also use L(tokens).unique() but below is idiomatic Python without the need for FastAI helpers\n",
    "vocab = set(tokens)\n",
    "\n",
    "# Convert to list just for display here. I am deliberately avoiding\n",
    "# some of the FastAI features just to remove as much magic as possible.\n",
    "len(vocab), list(vocab)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1ae6bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numericalization\n",
    "\n",
    "- The next step after tokenization is numericalization\n",
    "- This time we'll use the same approach as the PyTorch embeddings example (dictionaries)\n",
    "\n",
    "![numericalization](images/numericalization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "397f8b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#63095) [21,13,29,13,12,13,20,13,23,13...]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numericalize\n",
    "word2idx = {w : i for i,w in enumerate(vocab)}\n",
    "idx2word = {i : w for i,w in enumerate(vocab)}\n",
    "nums = L(word2idx[i] for i in tokens)\n",
    "nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14edd47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language Modeling\n",
    "\n",
    "- This is review\n",
    "- Predict next word based on previous three tokens\n",
    "- No recursion yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918821e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Language Modeling\n",
    "\n",
    "- Independent variable is three tokens\n",
    "- Dependent variable is a single token\n",
    "\n",
    "Example:\n",
    "```\n",
    "(['one', '.', 'two'], 'three'),\n",
    "(['.', 'three', '.'], '.'),\n",
    "(['four', '.', 'five'], 'six')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6a66b735",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#21031) [(['one', '.', 'two'], 'three'),(['.', 'three', '.'], '.'),(['four', '.', 'five'], 'six'),(['.', 'six', '.'], '.'),(['seven', '.', 'eight'], 'nine'),(['.', 'nine', '.'], '.'),(['ten', '.', 'eleven'], 'twelve'),(['.', 'twelve', '.'], '.'),(['thirteen', '.', 'fourteen'], 'fifteen'),(['.', 'fifteen', '.'], '.')...]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Producing the tri-grams\n",
    "# We did this in the embedding example.\n",
    "human_seqs_example = []\n",
    "for i in range(0, len(tokens) - 4, 3):\n",
    "    human_seqs_example.append((tokens[i:i+3], tokens[i+4]))\n",
    "    \n",
    "L(human_seqs_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "82b4bdd5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# nums is the numericalized text input\n",
    "# Also compress that for loop into a list comprehension because it was too readable.\n",
    "# Now it looks like we are a little smarter.\n",
    "seqs = L((tensor(nums[i:i+3]), nums[i + 3]) for i in range(0, len(nums) - 4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8adb2f13",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3]), torch.Size([64]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The cut variable is the cut between training and validation\n",
    "# We could have just kept them separate from the beginning.\n",
    "batch_size = 64\n",
    "cut = int(len(seqs) * 0.8)\n",
    "\n",
    "# Here the DataLoader will divide sequences into batches, not doing anything else.\n",
    "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=batch_size, shuffle=False)\n",
    "\n",
    "# Each batch is 64, [0] is x and is a tri-gram, [1] is y and is a single token index\n",
    "first(dls.train)[0].shape, first(dls.train)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e45b8379",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21, 13, 20, 13,  5, 13,  2, 13, 15, 13,  8, 13, 19, 13, 13, 13, 13, 13,\n",
       "        13, 13, 13, 13, 13, 17, 17, 17, 17, 17, 17, 17, 17, 17, 10, 21, 29, 12,\n",
       "        20, 23,  7,  5, 22, 16, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 18, 18,\n",
       "        18, 18, 18, 18, 18, 18, 18, 27, 21, 29])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = first(dls.train)[0]\n",
    "# First token across all batches\n",
    "x[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad934c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.com/fastai/fastbook/raw/e57e3155824c81a54f915edf9505f64d5ccdad84/images/att_00022.png\" width=\"800px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "152681a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class LMModel1(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.input_to_hidden = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.hidden_to_output = nn.Linear(n_hidden, vocab_sz)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First token across all batches\n",
    "        first_tokens = x[:, 0]\n",
    "        second_tokens = x[:, 1]\n",
    "        third_tokens = x[:, 2]\n",
    "        \n",
    "        # Get embedding for tokens\n",
    "        h = self.input_to_hidden(first_tokens)\n",
    "        h = self.hidden(h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        # Apply hidden layer to second token in batches\n",
    "        # Add the result.\n",
    "        h = h + self.input_to_hidden(second_tokens)\n",
    "        h = self.hidden(h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        h = h + self.input_to_hidden(third_tokens)\n",
    "        h = self.hidden(h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        return self.hidden_to_output(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "34146c7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.531881</td>\n",
       "      <td>1.803231</td>\n",
       "      <td>0.467792</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.381355</td>\n",
       "      <td>1.802319</td>\n",
       "      <td>0.468505</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.338977</td>\n",
       "      <td>1.719721</td>\n",
       "      <td>0.477300</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.278312</td>\n",
       "      <td>1.731577</td>\n",
       "      <td>0.490611</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.262202</td>\n",
       "      <td>1.659887</td>\n",
       "      <td>0.506061</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.237160</td>\n",
       "      <td>1.703498</td>\n",
       "      <td>0.506537</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.206963</td>\n",
       "      <td>1.777019</td>\n",
       "      <td>0.507012</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.199360</td>\n",
       "      <td>1.788651</td>\n",
       "      <td>0.507012</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.212504</td>\n",
       "      <td>1.738667</td>\n",
       "      <td>0.507012</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.197111</td>\n",
       "      <td>1.714087</td>\n",
       "      <td>0.507012</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit(10, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81163cb2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our first recurrent neural network\n",
    "\n",
    "- Simply refactors the previous network\n",
    "- Uses a for loop inside the forward function\n",
    "- Accuracy should be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bdf556",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.com/fastai/fastbook/raw/e57e3155824c81a54f915edf9505f64d5ccdad84/images/att_00070.png\" width=\"800px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "038974ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Module is the PyTorch base class for all networks.\n",
    "class LMModel2(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.input_to_hidden = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.hidden_to_output = nn.Linear(n_hidden, vocab_sz)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # what is with the h = 0?\n",
    "        # That wasn't introduced before\n",
    "        # Need to set h =0 explicitly because the loop has +=\n",
    "\n",
    "        h = 0\n",
    "        for i in range(3):\n",
    "            h = h + self.input_to_hidden(x[:, i])\n",
    "            h = F.relu(self.hidden(h))\n",
    "        return self.hidden_to_output(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "554ee4e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.410210</td>\n",
       "      <td>1.683014</td>\n",
       "      <td>0.478726</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.276662</td>\n",
       "      <td>1.611895</td>\n",
       "      <td>0.493463</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.223682</td>\n",
       "      <td>1.702572</td>\n",
       "      <td>0.499643</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.203796</td>\n",
       "      <td>1.760403</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.185061</td>\n",
       "      <td>1.890238</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.163429</td>\n",
       "      <td>2.255942</td>\n",
       "      <td>0.491562</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.243350</td>\n",
       "      <td>1.606940</td>\n",
       "      <td>0.506299</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.197510</td>\n",
       "      <td>1.793824</td>\n",
       "      <td>0.506774</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.145686</td>\n",
       "      <td>1.805490</td>\n",
       "      <td>0.506774</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.150602</td>\n",
       "      <td>2.046500</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, LMModel2(len(vocab), 128), loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit(10, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79cc4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maintaining State\n",
    "\n",
    "- We are not chaining together the sequences, maximum memory in the RNN is sequence length\n",
    "- Two problems\n",
    "    1. More practically, our network is now too deep (in the unrolled sense).\n",
    "       Again we aren't actually creating these layers, but still have to back-propagate.\n",
    "    2. In order for this to make sense, the next batch has to be the next in the sequence\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50578b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Back-Propagation Through Time (BPTT)\n",
    "\n",
    "- Solves the first problem\n",
    "- Tell PyTorch that we only want to update weights based on gradient from this batch\n",
    "- But we still don't through away the state we are passing along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "728fcc92",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class LMModel3(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.input_to_hidden = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.hidden_to_output = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(3):\n",
    "            self.h = self.h + self.input_to_hidden(x[:, i])\n",
    "            self.h = F.relu(self.hidden(self.h))\n",
    "        out = self.hidden_to_output(self.h)        \n",
    "        self.h = self.h.detach()\n",
    "        return out\n",
    "        \n",
    "    def reset(self):\n",
    "        self.h = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e802d10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Carefully arranging the batches\n",
    "\n",
    "- In the IMDB sentiment analysis example the `LMDataLoader` does this\n",
    "- We want the the batches to contain contiguous sequences\n",
    "- Normally shuffling is good, not here!\n",
    "- We can drop or pad the last batch to fit it in the tensor.\n",
    "- LMDataLoader pads, we we will drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "385c9795",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_seq_len = len(seqs) // batch_size\n",
    "batch_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "86873f9c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def group_chunks(ds, bs):\n",
    "    \"\"\"\n",
    "    ds: the dataset\n",
    "    \"\"\"\n",
    "    m = len(ds) // bs\n",
    "    new_ds = L()\n",
    "    for i in range(m):\n",
    "            new_ds += L(ds[i + m * j] for j in range(bs))\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "47e7d543",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Cut between training and validation\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(\n",
    "    # [0, cut)\n",
    "    group_chunks(seqs[:cut], batch_size),\n",
    "    \n",
    "    # [cut, end]\n",
    "    group_chunks(seqs[cut:], batch_size),\n",
    "    \n",
    "    bs=batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1fdfac07",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.672461</td>\n",
       "      <td>1.867919</td>\n",
       "      <td>0.479567</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.261106</td>\n",
       "      <td>1.722118</td>\n",
       "      <td>0.475962</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.110206</td>\n",
       "      <td>1.721647</td>\n",
       "      <td>0.502885</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.015317</td>\n",
       "      <td>1.721791</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.946742</td>\n",
       "      <td>1.728749</td>\n",
       "      <td>0.560337</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.912255</td>\n",
       "      <td>1.672993</td>\n",
       "      <td>0.575962</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.852893</td>\n",
       "      <td>1.664606</td>\n",
       "      <td>0.590625</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.798675</td>\n",
       "      <td>1.858297</td>\n",
       "      <td>0.580769</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.751387</td>\n",
       "      <td>1.832644</td>\n",
       "      <td>0.618510</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.732432</td>\n",
       "      <td>1.839459</td>\n",
       "      <td>0.616587</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top out at around 60%. A 10% improvement! That's actually pretty huge.\n",
    "# Without reset you get the error of trying to backward through the graph a second time...\n",
    "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(10, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b501a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create More Signal\n",
    "\n",
    "- Explains why in the IMDB example the x and y kept the entire sequence\n",
    "- Predict the entire sequence not just one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b87f1108",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sl = 16\n",
    "\n",
    "# Does this look readable to you?\n",
    "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0, len(nums)-sl-1,sl))\n",
    "cut = int(len(seqs) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e542f9c7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#16) ['one','.','two','.','three','.','four','.','five','.'...]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The independent variable\n",
    "L([idx2word[x.item()] for x in seqs[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b8f33b52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#16) ['.','two','.','three','.','four','.','five','.','six'...]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dependent variable (label)\n",
    "L([idx2word[x.item()] for x in seqs[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "64d2f6f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dsets(\n",
    "    group_chunks(seqs[:cut], batch_size),\n",
    "    group_chunks(seqs[cut:], batch_size),\n",
    "    bs=batch_size, drop_last=True, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "18d9460a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Modify the model so that it makes a prediction after every word\n",
    "class LMModel4(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, seq_len):\n",
    "        self.input_to_hidden = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.hidden_to_output = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = 0\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # outs will be a list of tensors that are shape (bs, vocab_sz)\n",
    "        outs = []\n",
    "        for i in range(self.seq_len):\n",
    "            self.h = self.h + self.input_to_hidden(x[:, i])\n",
    "            self.h = F.relu(self.hidden(self.h))\n",
    "            outs.append(self.hidden_to_output(self.h))\n",
    "        self.h = self.h.detach()\n",
    "        \n",
    "        # we want to return a tensor so stack our list of outs\n",
    "        # shape will be batch_size x sequence length x vocab size\n",
    "        return torch.stack(outs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2c8cab24",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# input shape is batch size x sequence len x vocab (output of neural net)\n",
    "# target shape is batch size x sequence len\n",
    "# Cross entropy can't handle a 2D tensor here,s o flatten (64x16x30, 64x16) to (1024x30, 1024)\n",
    "# That is, for all 1024 tokens in the batch of sequences list the 30 activations\n",
    "# Calculate loss against the correct token\n",
    "def loss_func(inp, targ):\n",
    "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e9e61c49",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.369873</td>\n",
       "      <td>2.017576</td>\n",
       "      <td>0.462321</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.759912</td>\n",
       "      <td>1.838556</td>\n",
       "      <td>0.461507</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.546585</td>\n",
       "      <td>1.792056</td>\n",
       "      <td>0.460124</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.429270</td>\n",
       "      <td>1.830774</td>\n",
       "      <td>0.457764</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.343087</td>\n",
       "      <td>1.768953</td>\n",
       "      <td>0.486816</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.264946</td>\n",
       "      <td>1.840395</td>\n",
       "      <td>0.489014</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.211312</td>\n",
       "      <td>1.788568</td>\n",
       "      <td>0.527588</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.155886</td>\n",
       "      <td>1.870630</td>\n",
       "      <td>0.508138</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.113114</td>\n",
       "      <td>1.874396</td>\n",
       "      <td>0.528564</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.069800</td>\n",
       "      <td>1.782308</td>\n",
       "      <td>0.539388</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.031517</td>\n",
       "      <td>1.732965</td>\n",
       "      <td>0.542155</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>1.684239</td>\n",
       "      <td>0.559082</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.945955</td>\n",
       "      <td>1.747047</td>\n",
       "      <td>0.562012</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.909381</td>\n",
       "      <td>1.727623</td>\n",
       "      <td>0.555094</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.883933</td>\n",
       "      <td>1.690879</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.842078</td>\n",
       "      <td>1.660708</td>\n",
       "      <td>0.581787</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.808791</td>\n",
       "      <td>1.674870</td>\n",
       "      <td>0.586833</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.777086</td>\n",
       "      <td>1.697493</td>\n",
       "      <td>0.582520</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.752854</td>\n",
       "      <td>1.679376</td>\n",
       "      <td>0.579590</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.730935</td>\n",
       "      <td>1.654423</td>\n",
       "      <td>0.580159</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.710362</td>\n",
       "      <td>1.675523</td>\n",
       "      <td>0.611409</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.683799</td>\n",
       "      <td>1.753995</td>\n",
       "      <td>0.588949</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.683350</td>\n",
       "      <td>1.658058</td>\n",
       "      <td>0.605225</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>1.636273</td>\n",
       "      <td>0.615479</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.634557</td>\n",
       "      <td>1.602770</td>\n",
       "      <td>0.612061</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.612014</td>\n",
       "      <td>1.557434</td>\n",
       "      <td>0.628174</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.601963</td>\n",
       "      <td>1.588026</td>\n",
       "      <td>0.619385</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.579982</td>\n",
       "      <td>1.574623</td>\n",
       "      <td>0.631673</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.566393</td>\n",
       "      <td>1.574490</td>\n",
       "      <td>0.624512</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.554316</td>\n",
       "      <td>1.546625</td>\n",
       "      <td>0.638346</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, LMModel4(len(vocab), 64, 16), loss_func=loss_func, metrics=accuracy)\n",
    "learn.fit(30, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "630853a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.496429</td>\n",
       "      <td>1.555763</td>\n",
       "      <td>0.637207</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.488470</td>\n",
       "      <td>1.565688</td>\n",
       "      <td>0.638835</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>1.577183</td>\n",
       "      <td>0.639486</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.483274</td>\n",
       "      <td>1.577752</td>\n",
       "      <td>0.640544</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.481578</td>\n",
       "      <td>1.578776</td>\n",
       "      <td>0.641846</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.480184</td>\n",
       "      <td>1.583196</td>\n",
       "      <td>0.642741</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.478654</td>\n",
       "      <td>1.580914</td>\n",
       "      <td>0.642904</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.476936</td>\n",
       "      <td>1.579383</td>\n",
       "      <td>0.643148</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.475016</td>\n",
       "      <td>1.581514</td>\n",
       "      <td>0.642904</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.473054</td>\n",
       "      <td>1.583959</td>\n",
       "      <td>0.643473</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.470913</td>\n",
       "      <td>1.578259</td>\n",
       "      <td>0.643473</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.469061</td>\n",
       "      <td>1.577773</td>\n",
       "      <td>0.643880</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.467088</td>\n",
       "      <td>1.577736</td>\n",
       "      <td>0.643962</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.465173</td>\n",
       "      <td>1.576098</td>\n",
       "      <td>0.646159</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.463282</td>\n",
       "      <td>1.577601</td>\n",
       "      <td>0.645752</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.461492</td>\n",
       "      <td>1.576634</td>\n",
       "      <td>0.646484</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.459683</td>\n",
       "      <td>1.575297</td>\n",
       "      <td>0.647461</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>1.574605</td>\n",
       "      <td>0.648844</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.456080</td>\n",
       "      <td>1.575231</td>\n",
       "      <td>0.649740</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.454310</td>\n",
       "      <td>1.577345</td>\n",
       "      <td>0.650879</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.452663</td>\n",
       "      <td>1.574519</td>\n",
       "      <td>0.652018</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.451034</td>\n",
       "      <td>1.573908</td>\n",
       "      <td>0.652588</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.449310</td>\n",
       "      <td>1.575353</td>\n",
       "      <td>0.653239</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.447648</td>\n",
       "      <td>1.572926</td>\n",
       "      <td>0.653809</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.445933</td>\n",
       "      <td>1.571275</td>\n",
       "      <td>0.654622</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.444200</td>\n",
       "      <td>1.571374</td>\n",
       "      <td>0.655029</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.442486</td>\n",
       "      <td>1.571820</td>\n",
       "      <td>0.655843</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.440835</td>\n",
       "      <td>1.572219</td>\n",
       "      <td>0.655843</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.439202</td>\n",
       "      <td>1.574384</td>\n",
       "      <td>0.656169</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.437686</td>\n",
       "      <td>1.572471</td>\n",
       "      <td>0.655680</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.436247</td>\n",
       "      <td>1.571506</td>\n",
       "      <td>0.656413</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.434828</td>\n",
       "      <td>1.571037</td>\n",
       "      <td>0.656169</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.433268</td>\n",
       "      <td>1.571885</td>\n",
       "      <td>0.656820</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.431828</td>\n",
       "      <td>1.573003</td>\n",
       "      <td>0.656576</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.430437</td>\n",
       "      <td>1.571304</td>\n",
       "      <td>0.656901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.429018</td>\n",
       "      <td>1.570813</td>\n",
       "      <td>0.657389</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.427529</td>\n",
       "      <td>1.568300</td>\n",
       "      <td>0.657715</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.426024</td>\n",
       "      <td>1.566638</td>\n",
       "      <td>0.658203</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.424576</td>\n",
       "      <td>1.566663</td>\n",
       "      <td>0.658284</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.423150</td>\n",
       "      <td>1.569210</td>\n",
       "      <td>0.658773</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.421668</td>\n",
       "      <td>1.572063</td>\n",
       "      <td>0.659424</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.420310</td>\n",
       "      <td>1.572413</td>\n",
       "      <td>0.658936</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.418993</td>\n",
       "      <td>1.578107</td>\n",
       "      <td>0.658610</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.417807</td>\n",
       "      <td>1.576983</td>\n",
       "      <td>0.659180</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.416580</td>\n",
       "      <td>1.578340</td>\n",
       "      <td>0.659098</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.415369</td>\n",
       "      <td>1.574568</td>\n",
       "      <td>0.659261</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.414061</td>\n",
       "      <td>1.572807</td>\n",
       "      <td>0.658040</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.412705</td>\n",
       "      <td>1.572689</td>\n",
       "      <td>0.658447</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.411409</td>\n",
       "      <td>1.574418</td>\n",
       "      <td>0.658203</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.410106</td>\n",
       "      <td>1.572371</td>\n",
       "      <td>0.658284</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(50, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a7b9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another 6%, pretty good."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
